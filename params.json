{
  "name": "Automated Essay Grading",
  "tagline": "By Anmol Gupta, Annie Hwang, Paul Lisker, and Kevin Loughlin",
  "body": "### Introduction\r\nOne of the main responsibilities of teachers and professors in the humanities is grading students essays [1]. Of course, manual essay grading for a classroom of students is a time-consuming process, and can even become tedious at times. Furthermore, essay grading can be plagued by inconsistencies in determining what a “good” essay really is.  Indeed, the grading of essays can be a topic of controversy, due to its intrinsic subjectivity. Instructors might be more inclined to better reward essays with a particular voice or writing style, or even a specific position on the essay prompt.\r\n\r\nWith these and other issues taken into consideration, the problem of essay grading is clearly a field ripe for a more systematic, unbiased method of rating written work.  There has been much research into creating AI agents, ultimately based on statistical models, that can automatically grade essays and therefore eliminate the potential for bias. Such a model would take typical features of strong essays into account, analyzing each essay for the presence (or lack) of these features.\r\n\r\nIn this project, which stems from an existing Kaggle competition sponsored by the William and Flora Hewlett Foundation [2], we have attempted to provide an efficient, automated solution to essay grading, thereby eliminating grader bias, as well as expediting a tedious and time-consuming job. While superior auto-graders that have resulted from years of extensive research surely exist, we feel that our final project demonstrates our ability to apply the data science process learned in this course to a complex, real-world problem.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}