{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/TODO:\n",
    "1. Standardize essay sets (each essay set should have mean 0 and st.dev of 1)\n",
    "2. Which crieterion do we use for evaluating our classifier?  I vote kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TODO if needed, include the words and stopwords imports\n",
    "# HOWEVER, to use them, you will need to download nltk stuff first if not done already\n",
    "# To do so, open a python shell (i.e. go to terminal and enter python), and then type\n",
    "#\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# After this, select the words and stopwords corpuses, and download them\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.corpus import words\n",
    "\n",
    "# Regular expressions might be useful\n",
    "import re\n",
    "\n",
    "# Beautiful soup might be useful\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for modeling\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "from sklearn.cross_validation import cross_val_predict \n",
    "from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   score  \n",
      "0    8.0  \n",
      "1    9.0  \n",
      "2    7.0  \n",
      "3   10.0  \n",
      "4    8.0  \n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "# Note that for essay set 2, score becomes average of 2 domain scores\n",
    "train_cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score']\n",
    "train_df = pd.read_csv('data/training_set_rel3.tsv', delimiter='\\t', usecols=train_cols,\n",
    "                       converters={'domain1_score': lambda x: float(x)})\n",
    "for i in xrange(train_df.shape[0]):\n",
    "    if not np.isnan(train_df.get_value(i, 'domain2_score')):\n",
    "        assert train_df.get_value(i, 'essay_set') == 2\n",
    "        new_val = (train_df.get_value(i, 'domain1_score') + train_df.get_value(i, 'domain2_score')) / 2.0\n",
    "        train_df.set_value(i, 'domain1_score', new_val) \n",
    "train_df = train_df.drop('domain2_score', axis=1)\n",
    "train_df = train_df.rename(columns={'domain1_score': 'score'})\n",
    "print train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in training set\n",
    "if train_df.isnull().any().any():\n",
    "    print 'Data is missing!'\n",
    "else:\n",
    "    print 'No missing data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
      "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
      "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
      "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
      "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
      "\n",
      "   score  \n",
      "0    7.0  \n",
      "1    8.0  \n",
      "2    9.0  \n",
      "3    9.0  \n",
      "4    9.0  \n"
     ]
    }
   ],
   "source": [
    "# Read in validation data\n",
    "valid_cols = ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n",
    "valid_df = pd.read_csv('data/valid_set.tsv', delimiter='\\t', usecols=valid_cols)\n",
    "valid_df['score'] = pd.Series([0.0] * valid_df.shape[0], index=valid_df.index)\n",
    "\n",
    "# scores are stored in separate data set, we'll put them in same one\n",
    "valid_scores = pd.read_csv('data/valid_sample_submission_5_column.csv', delimiter=',',\n",
    "                          converters={'predicted_score': lambda x: float(x)})\n",
    "\n",
    "# put each score in our data set, and make sure to handle essay set 2\n",
    "for i in xrange(valid_df.shape[0]):\n",
    "    dom1_predid = valid_df.get_value(i, 'domain1_predictionid')\n",
    "    row = valid_scores[valid_scores['prediction_id'] == dom1_predid]\n",
    "    score = row.get_value(row.index[0], 'predicted_score')\n",
    "    \n",
    "    dom2_predid = valid_df.get_value(i, 'domain2_predictionid')\n",
    "    if not np.isnan(dom2_predid):\n",
    "        assert valid_df.get_value(i, 'essay_set') == 2\n",
    "        rowB = valid_scores[valid_scores['prediction_id'] == dom2_predid]\n",
    "        scoreB = rowB.get_value(rowB.index[0], 'predicted_score')\n",
    "        score = (score + scoreB) / 2.0\n",
    "        \n",
    "    valid_df.set_value(i, 'score', score)\n",
    "        \n",
    "valid_df = valid_df.drop(['domain1_predictionid', 'domain2_predictionid'], axis=1)\n",
    "#print valid_df[valid_df['essay_set'] == 2].head()\n",
    "print valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in validation set\n",
    "if valid_df.isnull().any().any():\n",
    "    print 'Data is missing!'\n",
    "else:\n",
    "    print 'No missing data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returned a copy of old_df, with essays cleaned for count vectorizer\n",
    "def vectorizer_clean(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    for i in xrange(new_df.shape[0]):\n",
    "        new_df.set_value(i, 'essay', \" \".join(re.sub('[^a-zA-Z\\d\\s]', '', new_df['essay'].iloc[i]).lower().split())) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  dear local newspaper i think effects computers...   \n",
      "1         2          1  dear caps1 caps2 i believe that using computer...   \n",
      "2         3          1  dear caps1 caps2 caps3 more and more people us...   \n",
      "3         4          1  dear local newspaper caps1 i have found that m...   \n",
      "4         5          1  dear location1 i know having computers has a p...   \n",
      "\n",
      "   score  \n",
      "0    8.0  \n",
      "1    9.0  \n",
      "2    7.0  \n",
      "3   10.0  \n",
      "4    8.0  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_train = vectorizer_clean(train_df)\n",
    "print vectorizer_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  dear organization1 caps1 more and more people ...   \n",
      "1      1789          1  dear location1 time caps1 me tell you what i t...   \n",
      "2      1790          1  dear local newspaper have you been spending a ...   \n",
      "3      1791          1  dear readers caps1 you imagine how life would ...   \n",
      "4      1792          1  dear newspaper i strongly believe that compute...   \n",
      "\n",
      "   score  \n",
      "0    7.0  \n",
      "1    8.0  \n",
      "2    9.0  \n",
      "3    9.0  \n",
      "4    9.0  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_valid = vectorizer_clean(valid_df)\n",
    "print vectorizer_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_array = vectorizer_train.values[:, 2]\n",
    "\n",
    "#normalizing for y\n",
    "norm_y = vectorizer_train.groupby(['essay_set'])[['domain1_score']].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "norm_y.head()\n",
    "y = np.asarray(norm_y, dtype=\"|S6\")\n",
    "y_array = [x[0] for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "#Get all the text from data\n",
    "corpus = vectorizer_train['essay'].values\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=1)\n",
    "#Turn each text into an array of word counts\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "x = x.toarray()\n",
    "\n",
    "n_samples = len(x)\n",
    "train_indices = np.random.uniform(size=n_samples) > 1. / 3.  #Select two thirds for train\n",
    "\n",
    "x_train = x[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "x_test = x[~train_indices]\n",
    "y_test = y[~train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anniehwang/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "logistic = LogReg(penalty='l2', \n",
    "                    solver='liblinear', \n",
    "                    n_jobs=4)\n",
    "logistic.fit(x_train, y_train)\n",
    "\n",
    "#Print results\n",
    "y_pred = logistic.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy on overall test set: 0.39287456151\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy on overall test set:', cohen_kappa_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on overall test set: 0.424848658862\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "#normalizing for y\n",
    "norm_y = vectorizer_train.groupby(['essay_set'])[['domain1_score']].apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "y = np.asarray(norm_y, dtype=\"|S6\")\n",
    "y_array = [x[0] for x in y]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus, y_array, test_size=0.33, random_state=42)\n",
    "\n",
    "tfidf_model = Pipeline([('counts', vectorizer),\n",
    "                   ('tfidf', tfidf),\n",
    "                   ('regression', logistic), ])\n",
    "tfidf_model.fit(x_train, y_train)\n",
    "#tfidf_model.score(x_test, y_test)\n",
    "\n",
    "y_pred = tfidf_model.predict(x_test)\n",
    "print 'Accuracy on overall test set:', cohen_kappa_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
