{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/TODO:\n",
    "1. Standardize essay sets (each essay set should have mean 0 and st.dev of 1)\n",
    "2. Which crieterion do we use for evaluating our classifier?  I vote kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TODO if needed, include the words and stopwords imports\n",
    "# HOWEVER, to use them, you will need to download nltk stuff first if not done already\n",
    "# To do so, open a python shell (i.e. go to terminal and enter python), and then type\n",
    "#\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# After this, select the words and stopwords corpuses, and download them\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.corpus import words\n",
    "\n",
    "# Regular expressions might be useful\n",
    "import re\n",
    "\n",
    "# Beautiful soup might be useful\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   score  \n",
      "0    8.0  \n",
      "1    9.0  \n",
      "2    7.0  \n",
      "3   10.0  \n",
      "4    8.0  \n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "# Note that for essay set 2, score becomes average of 2 domain scores\n",
    "train_cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score']\n",
    "train_df = pd.read_csv('data/training_set_rel3.tsv', delimiter='\\t', usecols=train_cols,\n",
    "                       converters={'domain1_score': lambda x: float(x)})\n",
    "for i in xrange(train_df.shape[0]):\n",
    "    if not np.isnan(train_df.get_value(i, 'domain2_score')):\n",
    "        assert train_df.get_value(i, 'essay_set') == 2\n",
    "        new_val = (train_df.get_value(i, 'domain1_score') + train_df.get_value(i, 'domain2_score')) / 2.0\n",
    "        train_df.set_value(i, 'domain1_score', new_val) \n",
    "train_df = train_df.drop('domain2_score', axis=1)\n",
    "train_df = train_df.rename(columns={'domain1_score': 'score'})\n",
    "print train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in training set\n",
    "if train_df.isnull().any().any():\n",
    "    print 'Data is missing!'\n",
    "else:\n",
    "    print 'No missing data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
      "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
      "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
      "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
      "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
      "\n",
      "   score  \n",
      "0    7.0  \n",
      "1    8.0  \n",
      "2    9.0  \n",
      "3    9.0  \n",
      "4    9.0  \n"
     ]
    }
   ],
   "source": [
    "# Read in validation data\n",
    "valid_cols = ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n",
    "valid_df = pd.read_csv('data/valid_set.tsv', delimiter='\\t', usecols=valid_cols)\n",
    "valid_df['score'] = pd.Series([0.0] * valid_df.shape[0], index=valid_df.index)\n",
    "\n",
    "# scores are stored in separate data set, we'll put them in same one\n",
    "valid_scores = pd.read_csv('data/valid_sample_submission_5_column.csv', delimiter=',',\n",
    "                          converters={'predicted_score': lambda x: float(x)})\n",
    "\n",
    "# put each score in our data set, and make sure to handle essay set 2\n",
    "for i in xrange(valid_df.shape[0]):\n",
    "    dom1_predid = valid_df.get_value(i, 'domain1_predictionid')\n",
    "    row = valid_scores[valid_scores['prediction_id'] == dom1_predid]\n",
    "    score = row.get_value(row.index[0], 'predicted_score')\n",
    "    \n",
    "    dom2_predid = valid_df.get_value(i, 'domain2_predictionid')\n",
    "    if not np.isnan(dom2_predid):\n",
    "        assert valid_df.get_value(i, 'essay_set') == 2\n",
    "        rowB = valid_scores[valid_scores['prediction_id'] == dom2_predid]\n",
    "        scoreB = rowB.get_value(rowB.index[0], 'predicted_score')\n",
    "        score = (score + scoreB) / 2.0\n",
    "        \n",
    "    valid_df.set_value(i, 'score', score)\n",
    "        \n",
    "valid_df = valid_df.drop(['domain1_predictionid', 'domain2_predictionid'], axis=1)\n",
    "#print valid_df[valid_df['essay_set'] == 2].head()\n",
    "print valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in validation set\n",
    "if valid_df.isnull().any().any():\n",
    "    print 'Data is missing!'\n",
    "else:\n",
    "    print 'No missing data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returned a copy of old_df, with essays cleaned for count vectorizer\n",
    "def vectorizer_clean(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    for i in xrange(new_df.shape[0]):\n",
    "        new_df.set_value(i, 'essay', \" \".join(re.sub('[^a-zA-Z\\d\\s]', '', new_df['essay'].iloc[i]).lower().split())) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  dear local newspaper i think effects computers...   \n",
      "1         2          1  dear caps1 caps2 i believe that using computer...   \n",
      "2         3          1  dear caps1 caps2 caps3 more and more people us...   \n",
      "3         4          1  dear local newspaper caps1 i have found that m...   \n",
      "4         5          1  dear location1 i know having computers has a p...   \n",
      "\n",
      "   score  \n",
      "0    8.0  \n",
      "1    9.0  \n",
      "2    7.0  \n",
      "3   10.0  \n",
      "4    8.0  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_train = vectorizer_clean(train_df)\n",
    "print vectorizer_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  dear organization1 caps1 more and more people ...   \n",
      "1      1789          1  dear location1 time caps1 me tell you what i t...   \n",
      "2      1790          1  dear local newspaper have you been spending a ...   \n",
      "3      1791          1  dear readers caps1 you imagine how life would ...   \n",
      "4      1792          1  dear newspaper i strongly believe that compute...   \n",
      "\n",
      "   score  \n",
      "0    7.0  \n",
      "1    8.0  \n",
      "2    9.0  \n",
      "3    9.0  \n",
      "4    9.0  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_valid = vectorizer_clean(valid_df)\n",
    "print vectorizer_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
