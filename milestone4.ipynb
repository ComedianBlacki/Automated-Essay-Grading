{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES/TODO:\n",
    "1. How do we deal with standardized scores in model evaluation?\n",
    "2. Which criterion do we use for evaluating our classifier? (Quadratic weighted kappa or spearman's correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# TODO if needed, include the words and stopwords imports\n",
    "# HOWEVER, to use them, you will need to download nltk stuff first if not done already\n",
    "# To do so, open a python shell (i.e. go to terminal and enter python), and then type\n",
    "#\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# After this, select the words and stopwords corpuses, and download them\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.corpus import words\n",
    "\n",
    "# Regular expressions might be useful\n",
    "import re\n",
    "\n",
    "# Beautiful soup might be useful\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# for modeling\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "# from sklearn.cross_validation import cross_val_predict \n",
    "# from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_regularized_scores(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    new_df['std_score'] = new_df.groupby(['essay_set'])[['score']].apply(lambda x: (x - np.mean(x)) / (np.std(x)))\n",
    "    return new_df\n",
    "\n",
    "def create_regularization_data(old_df):\n",
    "    #getting the number of datasets\n",
    "    max_essay_set = max(old_df['essay_set'])\n",
    "    #list of the regularized values\n",
    "    regularization_data = []\n",
    "    for i in range(max_essay_set+1):\n",
    "        mean = np.mean((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        std = np.std((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        regularization_data.append([i + 1, mean, std])\n",
    "    return regularization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularized data for each essay set =  [[1, 8.528323051037576, 1.5381336495587767], [2, 6.749444444444444, 1.3844371990179603], [3, 1.8482039397450754, 0.8149207612821795], [4, 1.4322033898305084, 0.9395167668768533], [5, 2.4088642659279778, 0.9705520523317599], [6, 2.72, 0.970360757656664], [7, 16.062460165710643, 4.583888354164165], [8, 36.95020746887967, 5.749521294509325], [9, nan, nan]]\n",
      "\n",
      "\n",
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   score  std_score  \n",
      "0      8  -0.343483  \n",
      "1      9   0.306655  \n",
      "2      7  -0.993622  \n",
      "3     10   0.956794  \n",
      "4      8  -0.343483  \n",
      "\n",
      "\n",
      "mean and standard deviation of essay set 1 =  6.29333937739e-16 , 1.0\n",
      "mean and standard deviation of essay set 2 =  1.88614556072e-16 , 1.0\n",
      "mean and standard deviation of essay set 3 =  -8.54215629607e-17 , 1.0\n",
      "mean and standard deviation of essay set 4 =  -1.33038589561e-16 , 1.0\n",
      "mean and standard deviation of essay set 5 =  9.93357443086e-18 , 1.0\n",
      "mean and standard deviation of essay set 6 =  -5.91378797784e-16 , 1.0\n",
      "mean and standard deviation of essay set 7 =  1.32002616472e-16 , 1.0\n",
      "mean and standard deviation of essay set 8 =  -2.54905977991e-17 , 1.0\n"
     ]
    }
   ],
   "source": [
    "# Read in training data\n",
    "# Note that for essay set 2, score becomes average of 2 domain scores\n",
    "train_cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score']\n",
    "train_df = pd.read_csv('data/training_set_rel3.tsv', delimiter='\\t', usecols=train_cols)\n",
    "for i in xrange(train_df.shape[0]):\n",
    "    if not np.isnan(train_df.get_value(i, 'domain2_score')):\n",
    "        assert train_df.get_value(i, 'essay_set') == 2\n",
    "        new_val = train_df.get_value(i, 'domain1_score') + train_df.get_value(i, 'domain2_score')\n",
    "        train_df.set_value(i, 'domain1_score', new_val) \n",
    "train_df = train_df.drop('domain2_score', axis=1)\n",
    "train_df = train_df.rename(columns={'domain1_score': 'score'})\n",
    "\n",
    "################\n",
    "regularization_data = create_regularization_data(train_df)\n",
    "train_df = append_regularized_scores(train_df)\n",
    "\n",
    "print \"The regularized data for each essay set = \", regularization_data\n",
    "print \"\\n\"\n",
    "\n",
    "#print train_df[train_df['essay_set'] == 2].head()\n",
    "print train_df.head()\n",
    "print \"\\n\"\n",
    "\n",
    "#validate that the standardization works\n",
    "max_essay_set = max(train_df['essay_set'])\n",
    "for i in range (max_essay_set):\n",
    "    valid = train_df[train_df[\"essay_set\"] == i + 1][\"std_score\"]\n",
    "    print \"mean and standard deviation of essay set \" + str(i + 1) + \" = \", np.mean(valid), \",\", np.std(valid)\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing training data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in training set\n",
    "if train_df.isnull().any().any():\n",
    "    print 'Training data is missing!'\n",
    "else:\n",
    "    print 'No missing training data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
      "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
      "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
      "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
      "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
      "\n",
      "   score  \n",
      "0      7  \n",
      "1      8  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n"
     ]
    }
   ],
   "source": [
    "# Read in validation data\n",
    "valid_cols = ['essay_id', 'essay_set', 'essay', 'domain1_predictionid', 'domain2_predictionid']\n",
    "valid_df = pd.read_csv('data/valid_set.tsv', delimiter='\\t', usecols=valid_cols)\n",
    "valid_df['score'] = pd.Series([0] * valid_df.shape[0], index=valid_df.index)\n",
    "\n",
    "# scores are stored in separate data set, we'll put them in same one\n",
    "valid_scores = pd.read_csv('data/valid_sample_submission_5_column.csv', delimiter=',')\n",
    "\n",
    "# put each score in our data set, and make sure to handle essay set 2\n",
    "for i in xrange(valid_df.shape[0]):\n",
    "    dom1_predid = valid_df.get_value(i, 'domain1_predictionid')\n",
    "    row = valid_scores[valid_scores['prediction_id'] == dom1_predid]\n",
    "    score = row.get_value(row.index[0], 'predicted_score')\n",
    "    \n",
    "    dom2_predid = valid_df.get_value(i, 'domain2_predictionid')\n",
    "    if not np.isnan(dom2_predid):\n",
    "        assert valid_df.get_value(i, 'essay_set') == 2\n",
    "        rowB = valid_scores[valid_scores['prediction_id'] == dom2_predid]\n",
    "        scoreB = rowB.get_value(rowB.index[0], 'predicted_score')\n",
    "        score += scoreB\n",
    "        \n",
    "    valid_df.set_value(i, 'score', score)\n",
    "        \n",
    "valid_df = valid_df.drop(['domain1_predictionid', 'domain2_predictionid'], axis=1)\n",
    "#print valid_df[valid_df['essay_set'] == 2].head()\n",
    "print valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing validation data!\n"
     ]
    }
   ],
   "source": [
    "# Show nothing is empty in validation set\n",
    "if valid_df.isnull().any().any():\n",
    "    print 'Validation data is missing!'\n",
    "else:\n",
    "    print 'No missing validation data!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returned a copy of old_df, with essays cleaned for count vectorizer\n",
    "# cleaning returns essay with only lowercase words separated by space\n",
    "def vectorizer_clean(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    for i in xrange(new_df.shape[0]):\n",
    "        new_df.set_value(i, 'essay', \" \".join(re.sub('[^a-zA-Z\\d\\s]', '', new_df['essay'].iloc[i]).lower().split())) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  dear local newspaper i think effects computers...   \n",
      "1         2          1  dear caps1 caps2 i believe that using computer...   \n",
      "2         3          1  dear caps1 caps2 caps3 more and more people us...   \n",
      "3         4          1  dear local newspaper caps1 i have found that m...   \n",
      "4         5          1  dear location1 i know having computers has a p...   \n",
      "\n",
      "   score  std_score  \n",
      "0      8  -0.343483  \n",
      "1      9   0.306655  \n",
      "2      7  -0.993622  \n",
      "3     10   0.956794  \n",
      "4      8  -0.343483  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_train = vectorizer_clean(train_df)\n",
    "print vectorizer_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0      1788          1  dear organization1 caps1 more and more people ...   \n",
      "1      1789          1  dear location1 time caps1 me tell you what i t...   \n",
      "2      1790          1  dear local newspaper have you been spending a ...   \n",
      "3      1791          1  dear readers caps1 you imagine how life would ...   \n",
      "4      1792          1  dear newspaper i strongly believe that compute...   \n",
      "\n",
      "   score  \n",
      "0      7  \n",
      "1      8  \n",
      "2      9  \n",
      "3      9  \n",
      "4      9  \n"
     ]
    }
   ],
   "source": [
    "# print essays cleaned for vectorizer (essay is now just lowercase words separated by space) \n",
    "vectorizer_valid = vectorizer_clean(valid_df)\n",
    "print vectorizer_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-0.343' '0.3066' '-0.993' '0.9567' '-0.343']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(stop_words = 'english', ngram_range=(2,2))\n",
    "vectorizer3 = TfidfVectorizer(stop_words = 'english', ngram_range=(3,3))\n",
    "vectorizer4 = TfidfVectorizer(stop_words = 'english', ngram_range=(4,4))\n",
    "vectorizer5 = TfidfVectorizer(stop_words = 'english', ngram_range=(5,5))\n",
    "\n",
    "\n",
    "#Get all the text from data\n",
    "train_essays = vectorizer_train['essay'].values\n",
    "\n",
    "#Turn each text into an array of word counts\n",
    "train_vectors = vectorizer.fit_transform(train_essays).toarray()\n",
    "\n",
    "train_vectors2 = vectorizer2.fit_transform(train_essays).toarray()\n",
    "train_vectors3 = vectorizer3.fit_transform(train_essays).toarray()\n",
    "train_vectors4 = vectorizer4.fit_transform(train_essays).toarray()\n",
    "train_vectors5 = vectorizer5.fit_transform(train_essays).toarray()\n",
    "\n",
    "\n",
    "#normalizing for y\n",
    "train_std_scores = np.asarray(vectorizer_train['std_score'], dtype=\"|S6\")\n",
    "print train_std_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "## TfidfVectorizer with ngram=(1,1) ##\n",
    "######################################\n",
    "\n",
    "\n",
    "###############\n",
    "# Logistic L2 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L2 penalty\n",
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear', n_jobs=4)\n",
    "logistic_l2.fit(train_vectors, train_std_scores)\n",
    "\n",
    "valid_vectors = vectorizer.transform(vectorizer_valid['essay'].values).toarray()\n",
    "\n",
    "# My guess is we will want to denormalize these scores for quadratic weighted k\n",
    "valid_pred_std_scores_l2 = logistic_l2.predict(valid_vectors)\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df[\"Log_L2 predicted_scores\"] = valid_pred_std_scores_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l2 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L2 predicted_scores']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l2.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l2\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l2'] = stand_pred_values_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# Logistic L1 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L1 penalty\n",
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear', n_jobs=4)\n",
    "logistic_l1.fit(train_vectors, train_std_scores)\n",
    "\n",
    "valid_pred_std_scores_l1 = logistic_l1.predict(valid_vectors)\n",
    "\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df['Log_L1 predicted_scores'] = valid_pred_std_scores_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l1 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L1 predicted_scores']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l1.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l1\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l1'] = stand_pred_values_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC L2\n",
      "Number of correct predictions = 1282\n",
      "Total number of observations = 4218\n",
      "Score = 0.303935514462\n",
      "\n",
      "LOGISTIC L1\n",
      "Number of correct predictions = 1320\n",
      "Total number of observations = 4218\n",
      "Score = 0.312944523471\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "#   Scoring   #\n",
    "###############\n",
    "\n",
    "#Scoring the predicted values with the actual values\n",
    "log_l2_count = 0\n",
    "log_l1_count = 0\n",
    "for i in range(len(valid_df)):\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l2']:\n",
    "        log_l2_count += 1\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l1']:\n",
    "        log_l1_count += 1\n",
    "        \n",
    "print \"LOGISTIC L2\"\n",
    "print \"Number of correct predictions =\", log_l2_count\n",
    "print \"Total number of observations =\", len(valid_df)\n",
    "print \"Score =\", float(log_l2_count) / len(valid_df)\n",
    "\n",
    "print \"\"\n",
    "print \"LOGISTIC L1\"\n",
    "print \"Number of correct predictions =\", log_l1_count\n",
    "print \"Total number of observations =\", len(valid_df)\n",
    "print \"Score =\", float(log_l1_count) / len(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L2: SpearmanrResult(correlation=0.91959718560647619, pvalue=0.0)\n",
      "Logistic L1: SpearmanrResult(correlation=0.9186645533963379, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "#Spearman Correlation Coefficient\n",
    "from scipy.stats import spearmanr as Spearman\n",
    "\n",
    "print \"Logistic L2:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l2\"])\n",
    "print \"Logistic L1:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, as we expand ngram length to 2, we see that the computation power required becomes such that the kernel dies.  Therefore, we will have to try limiting the number of words included in the vectorizer for our future models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "## TfidfVectorizer with ngram=(2,2) ##\n",
    "######################################\n",
    "\n",
    "###############\n",
    "# Logistic L2 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L2 penalty\n",
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear', n_jobs=4)\n",
    "logistic_l2.fit(train_vectors2, train_std_scores)\n",
    "\n",
    "valid_vectors2 = vectorizer2.transform(vectorizer_valid['essay'].values).toarray()\n",
    "\n",
    "# My guess is we will want to denormalize these scores for quadratic weighted k\n",
    "valid_pred_std_scores_l2 = logistic_l2.predict(valid_vectors2)\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df[\"Log_L2 predicted_scores_2\"] = valid_pred_std_scores_l2\n",
    "\n",
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l2 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L2 predicted_scores_2']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l2.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l2\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l2_2'] = stand_pred_values_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# Logistic L1 #\n",
    "###############\n",
    "\n",
    "# Logistic Model with L1 penalty\n",
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear', n_jobs=4)\n",
    "logistic_l1.fit(train_vectors2, train_std_scores)\n",
    "\n",
    "valid_pred_std_scores_l1 = logistic_l1.predict(valid_vectors2)\n",
    "\n",
    "\n",
    "# Appending predicted scores to validation data set\n",
    "valid_df['Log_L1 predicted_scores_2'] = valid_pred_std_scores_l1\n",
    "\n",
    "#denormalizing the values and placing them into the stand_pred_values array\n",
    "stand_pred_values_l1 = []\n",
    "for i in range(max_essay_set):\n",
    "    current_set = valid_df[valid_df['essay_set'] == i + 1]['Log_L1 predicted_scores_2']\n",
    "    for value in current_set:\n",
    "        stand_pred_values_l1.append(int(float(value) * float(regularization_data[i][2]) + (regularization_data[i][1])))\n",
    "# print stand_pred_values_l1\n",
    "\n",
    "#adding the denormalizede predicted values to the valid_df dataset\n",
    "valid_df['newly_predicted_scores_log_l1_2'] = stand_pred_values_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#   Scoring   #\n",
    "###############\n",
    "\n",
    "#Scoring the predicted values with the actual values\n",
    "log_l2_count_2 = 0\n",
    "log_l1_count_2 = 0\n",
    "for i in range(len(valid_df)):\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l2_2']:\n",
    "        log_l2_count += 1\n",
    "    if valid_df.iloc[i]['score'] == valid_df.iloc[i]['newly_predicted_scores_log_l1_2']:\n",
    "        log_l1_count += 1\n",
    "        \n",
    "print \"LOGISTIC L2\"\n",
    "print \"Number of correct predictions =\", log_l2_count_2\n",
    "print \"Total number of observations =\", len(valid_df)\n",
    "print \"Score =\", float(log_l2_count_2) / len(valid_df)\n",
    "\n",
    "print \"\"\n",
    "print \"LOGISTIC L1\"\n",
    "print \"Number of correct predictions =\", log_l1_count_2\n",
    "print \"Total number of observations =\", len(valid_df)\n",
    "print \"Score =\", float(log_l1_count_2) / len(valid_df)\n",
    "\n",
    "#Spearman Correlation Coefficient\n",
    "from scipy.stats import spearmanr as Spearman\n",
    "\n",
    "print \"Logistic L2:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l2_2\"])\n",
    "print \"Logistic L1:\", Spearman(a = valid_df[\"score\"], b = valid_df[\"newly_predicted_scores_log_l1_2\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
